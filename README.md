Hello World!
This is the annotated Annotated Transformer.
The original Transformer paper needs no introduction.
The Annotated Transformer codebase is originally from Harvard that implements the paper with annotation.
This notebook is a fork of this repository but with additional expansions from me, including:
* Details & design choice of the Transformer architecture 
* Takeaways from related paper
* Pytorch syntax
* Extending the project to more language modeling tasks
* FAQs that I came up with as I studied the paper.
Making this public so more students of deep learning can benefit from my trial and error (i.e. stochastic gradient descent).
