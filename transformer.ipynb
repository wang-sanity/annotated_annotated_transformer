{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Annotated *Annotated Transformer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is the annotated *Annotated Transformer*.   \n",
    "The original *Transformer* paper needs no introduction.  \n",
    "The *Annotated Transformer* codebase is originally from Harvard that implements the paper.    \n",
    "I expanded on some of the concepts mentioned in the *Annotated Transformer*, takeaways from related paper, pytorch syntax, and extended the project to more language modeling tasks. Also I included answers to some of my own questions that I came up with as I studied the paper.  \n",
    "Making this public so more students of deep learning can benefit from my trial and error (i.e. stochastic gradient descent). \n",
    "\n",
    "Overall this notebook is divided into the following sections:\n",
    "* Transformer Model\n",
    "* Data Pipeline\n",
    "* Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency is managed via conda through an environment file (env.yml)\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import altair as alt # for visualization\n",
    "import pandas as pd\n",
    "import spacy # for tokenization\n",
    "\n",
    "import torch\n",
    "from torchtext.data.functional import to_map_style_dataset # from iterator to map object\n",
    "import torchtext.datasets as datasets\n",
    "from torchtext.vocab import build_vocab_from_iterator # given tokenizer, language, return vocab (i.e. dictionary)\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation for the Transformer\n",
    "* Seq2Seq and other RNN language models achieved SOTA results at the time with an encoder-decoder architecture\n",
    "* However RNN is hard to parallelize with O(n) operations during training; also computing a single context vector would result in loss in information, especially as the context window becomes large (since the information of the initial token needs to go through O(n) layer to reach the nth token; this makes backprop hard too) \n",
    "* Attention mechanism is proposed as a way to re-compute the context vector depending on the token being decoded. This goes one step further than the word2vec idea - not only can a word be represented by the words around it on average, this representation can be further refined conditional on the current sentence\n",
    "* Now comes the idea *Attention is all you need* - we don't even need the RNN part of the model. Instead we use attention coupled with multi-head mechanism + positional encoding to process a full sequence at once - this is O(1) for one sequence \n",
    "* Another keyword here is \"self-attention\" - this is cool as all the latent representation comes from the original sequence itself in the encoder and decoder. Cross attention is used when results from the encoder and decoder are compared\n",
    "\n",
    "\n",
    "### Key design\n",
    "How does the Encoder - Decoder architecture work?   \n",
    "* Encoder produces a representation of the input sentence  \n",
    "* Decoder takes in this representation, and the decoded output so far, and produces the next token prediction autoregressively  \n",
    "* There are other architectures later on that use decoder / encoder only architectures - more to come on these later (#TODO)\n",
    "  \n",
    "What is in the Encoder vs. Decoder?\n",
    "* Encoder: Embedding => Positional encoding => Attention layer => Feedforward network  \n",
    "* Decoder: same as the encoder except the attention layer also takes in *final* output from the encoder \n",
    "\n",
    "What is the src mask vs. target mask? Why do we need them?\n",
    "* One is applied to the input and the other the output\n",
    "* One reason masks are needed is to replace padding with -inf so no attention is paid to them in the attention layer - this applies to both the src and tgt masks\n",
    "* A different mask is needed for the target - specifically, when trying to predict the token at position i, only y[:i] is displayed to prevent leakage. To handle the special case of producing the first token, a special start_of_sentence token is pre-pended to the target\n",
    "* This effectively produces a matrix where the upper triangular area gets masked out\n",
    " \n",
    "Is there a difference in the src and tgt embedding?\n",
    "* Some research has showed that using the same embeddings results in better performance as the src embedding converges to the tgt embedding\n",
    "* In the case where there are two languages, two separate embeddings may make more sense\n",
    "\n",
    "What is the generator?\n",
    "* Generator projects the transformer's output to vocab space and generates the softmax probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    # overall, think of there being only two components. The encoder encodes and the decoder decodes\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        encoded_output = self.encode(src, src_mask)\n",
    "        decoded_output = self.decode(encoded_output, src_mask, tgt, tgt_mask) # cross attention needs src_mask\n",
    "        return decoded_output\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "\n",
    "# alternatively can put the generator in the encoder decoder module as another function\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim = -1) # log_softmax numerically more stable than the softmax and usable with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder in detail\n",
    "Encoder: a stack of N=6 identical layers\n",
    "\n",
    "- encoder_layer_0\n",
    "    - sublayer_0 attention\n",
    "        - layer norm\n",
    "        - attention\n",
    "        - dropout \n",
    "        - residual connection\n",
    "    - sublayer_1 feedforward\n",
    "        - layer norm\n",
    "        - feedforward\n",
    "        - dropout\n",
    "        - residual connection\n",
    "- encoder_layer_1  \n",
    "...  \n",
    "- layer norm\n",
    "\n",
    "What is batch norm? (in comparison to layer norm)\n",
    "* First it normalizes the statistics in a given batch so that given a feature, the batch has a mean of 0 and standard deviation of 1. \n",
    "* Then two learnable parameters gamma (scale) and beta (shift) to learn any distribution\n",
    "* It was introduced as a way to improve network stability by reducing \"covariate shift\" - the stability in turn allows for higher learning rate\n",
    "* Typically added in between the linear component and the non-linear component of a network\n",
    "\n",
    "What is layer norm?\n",
    "* Layer norm normalizes all features in a given example / set of examples such as they have mean 0 and standard deviation of 1\n",
    "* Similarly it has two learnable parameters to shift the distribution\n",
    "* This is useful when the batch of data is small\n",
    "* In RNN, batch norm struggles as it disrupts dependencies between time steps. In the transformer, each sequence is also processed independently\n",
    "\n",
    "What is drop out?  \n",
    "* Some neurons' weights are set to 0, which allows the network to learn redundant representation\n",
    "\n",
    "How does these techniques behave differently during training vs. inference?  \n",
    "* Batch norm: an EMA mean and std are cached during to be used for inference (since inference only has 1 instance)\n",
    "* Layer norm: behaves the same\n",
    "* Dropout: scaling the activation by 1/p during training so that the expected value remains the same at inference; alternatively, activation is scaled by p during inference\n",
    "\n",
    "What is the purpose of residual connection?  \n",
    "* This was originally introduced as part of the paper *Deep Residual Learning for Image Recognition*\n",
    "* Structurally the output of a sublayer is added to the input of the sublayer, and go through ReLU activation again\n",
    "* In the extreme, this allows for an identity function to be learned quickly, speeding up training in deep network\n",
    "* One intuition is that a deeper network should act strictly better than a shallow one, as the additional layers can be just identity functions - this is not empricially observed however\n",
    "* Instead of learning the mapping between input and output, now the network learns the difference between the input and output (residual mapping)\n",
    "* The residual mapping can easily be learned to be 0, which is the identity function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an encoder consists of N encoder_layers\n",
    "# each encoder_layer contain a self-attention sublayer and feedforward sublayer\n",
    "# Each sublayer contains layer norm, dropout and residual connection\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N) # replicate given layer n times \n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask): # need to have the mask defined here to handle different masks instead of __init__\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "\n",
    "class SubLayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer): # notice x is the input whereas sublayer is a function\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout): # dropout is a scalar here\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SubLayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"learned scaling factor. prevent the gradient vanishing / exploding problems\"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim = True) # [example_cnt, seq_length, embedding_size] => take the mean across features; dont collapse the matrix\n",
    "        std = x.std(-1, keepdim = True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) # module list, pretty similar to Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder in detail\n",
    "What is the input and output for a decoder's prediction?\n",
    "* The input is the same source sentence and the target sequence up to the token being predicted\n",
    "* The target is the token being predicted \n",
    "* Teacher forcing is used during the training where the correct target sequence is used rather than what's previously generated by the model\n",
    "\n",
    "\n",
    "Why do we need source mask in the decoder if the encoder memory input is already affected by it?  \n",
    "*  The output of the encoder contains layer norm layer, so source mask needs to be re-applied\n",
    "\n",
    "Why is the subsequent mask size (1, seq_length, seq_length) and how is it applied?  \n",
    "*  This is applied to the attention matrix of size (example_count, seq_length, seq_length) to mask out the upper triangular area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a decoder consists of N decoder layer\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn # attention(query, key, value, mask)\n",
    "        self.src_attn = src_attn # attention(query, key, value, mask)\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SubLayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) # self attention\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) # cross attention\n",
    "        return self.sublayer[2](x, self.feed_forward) # feedforward layer\n",
    "    \n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size) # [1, seq_length, seq_length]\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    ) \n",
    "    return subsequent_mask == 0 # diagnoals are True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention in detail\n",
    "Attention maps a query and key-value pairs into an output.   \n",
    "A compatibility function between the query and key determines which value to output.  \n",
    "The output is a weighted sum of the values.  \n",
    "\n",
    "What is the complexity of the attention mechanism?\n",
    "* query@key: each cell takes d computation, there are n**2 cells, so O(n^2d)\n",
    "* softmax: each cell takes n computation, there are n**2 cells, so O(n^2)\n",
    "* value: [n,n]@[n,d] => each cell takes n, there are n*d cells, so O(n^2d)\n",
    "* overall, O(n^2d)\n",
    "\n",
    "What is multi-head attention?\n",
    "* Instead of learning one representation of key, query, and value, we learn h different representations\n",
    "* This allows the model to focus on different parts of the sequence\n",
    "* The h outputs are concatenated and linearly transformed to the expected dimension\n",
    "\n",
    "How does dimension of the different matrices change in multi-head attention?\n",
    "* Q: [cnt,n,d], K: [cnt,n,d], V: [cnt,n,d]\n",
    "* multi-head: for {Q,K,V}: [cnt,n,d]@[cnt,d,d] => [cnt,n,d]; => [cnt,n,d] reshape => [cnt,n,h,d/h] transpose => [cnt,h,n,d/h] transpose => [cnt,n,h,d/h] reshape => [cnt,n,d]  \n",
    "\n",
    "How come there is no non-linearity (e.g. ReLU) in the attention mechanism?  \n",
    "* Not needed to model dependencies between the query and key; also without relu it's more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None): # dropout is a function this time\n",
    "    d_k = query.size(-1) # embedding size\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(d_k) # [count, n, d] @ [count, d, n] => [count, n, n]\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9) # minus infinity => dot product becomes very negative => close to 0 from softmax\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn # return both the weighted value, as well as the probability\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4) # four separate linear models for q / k / v / output\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # [cnt, n, n] => [cnt, 1, n, n]\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        # said differently...\n",
    "        # query = self.linears[0](query).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # key = self.linears[1](key).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # value = self.linears[2](value).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # [cnt, h, n, d/h]\n",
    "\n",
    "        # note this implementation is slightly different from simple concatenation - attention is processed as one matrix\n",
    "        x = (\n",
    "            x.transpose(1,2)\n",
    "            .contiguous() # make sure the memory is contiguous for ops like view and performance\n",
    "            .view(nbatches, -1, self.h * self.d_k) \n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        \n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Positional encoding  \n",
    "Embedding is really just a lookup table taking in [n, vocab] and returning [n, d]  \n",
    "The biggest takeaway from the positional encoding is that the model can learn the relative position of the tokens, but not the absolute position. This is due to periodicity of the sin and cosine function. e.g. f(pos+k) -f(pos) = f(pos+2k) - f(pos+k)\n",
    "\n",
    "Dropout is used for positional encoding, but not for embedding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model) # lookup table\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model) # maintain the variance of the embedding \n",
    "    \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model) # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1) # [max_len, 1]\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # [1, max_len, d_model]\n",
    "        self.register_buffer(\"pe\", pe) # register as a parameter without backprop\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False) # notice no gradient calc is needed\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "\n",
    "\n",
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    # encoder, decoder, src_embed, tgt_embed, generator\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)), # a sequential module is defined here directly\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab)\n",
    "    )\n",
    "\n",
    "    # xavier initialization\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p) # make training faster\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple inference test - predict without feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test():\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    test_model.eval()\n",
    "\n",
    "    src = torch.tensor([[1,2,3,4,5,6,7,8,9,10]])\n",
    "    src_mask = torch.ones(1,1,10)\n",
    "\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "\n",
    "    ys = torch.zeros(1,1).type_as(src) # [1,1]\n",
    "    \n",
    "    for i in range(20):\n",
    "        out = test_model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = test_model.generator(out[:,-1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1,1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    print(f\"prediction {ys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple copy paste model\n",
    "\n",
    "Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch_size, nbatches):\n",
    "    \"\"\"Generate random data for a src-tgt copy task.\n",
    "    V: vocab size\n",
    "    \"\"\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch_size, 10)) # [count, vocab]\n",
    "        data[:, 0] = 1 # SOS == 1\n",
    "        src = data.requires_grad_(False).clone().detach()\n",
    "        tgt = data.requires_grad_(False).clone().detach()\n",
    "        yield Batch(src, tgt, 0) # src, tgt, padding value\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2) # [count,1,vocab]\n",
    "        if tgt is not None:            \n",
    "            self.tgt = tgt[:, :-1] # skip the last one EOS [count, vocab-1]\n",
    "            self.tgt_y = tgt[:, 1:] # skip the first one SOS [count, vocab-1]\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad) # [count, vocab-1, vocab-1]\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum() # count of non-padding token\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2) # [count,1,vocab-1] \n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        ) # [count,1,vocab-1] & [1,vocab-1,vocab-1]\n",
    "        return tgt_mask # [count, vocab-1, vocab-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "\n",
    "What is label smoothing?  \n",
    "* Label smoothing is a regularization technique that prevents the model from being too confident about the prediction\n",
    "* We can use it as the criterion instead of cross entropy loss\n",
    "* Specifically the target distribution is smoothed to be uniform over the vocabulary\n",
    "* The target becomes 1-alpha, and the rest becomes alpha/(vocab_size-1)\n",
    "\n",
    "What is KL divergence loss vs. Cross entropy loss?  \n",
    "* KL divergence is often used for comparing two probability distributions\n",
    "* With label smoothing, KL divergence could be a better fit \n",
    "* Entropy + KL divergence = cross entropy\n",
    "* KL can be less stable than cross entropy due to the inclusion of entropy term (unstable with mini-batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size # vocab\n",
    "        true_dist = x.data.clone() # [count, vocab]\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "\n",
    "        mask = torch.nonzero(target.data == self.padding_idx) # return padding indices\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        \n",
    "        return self.criterion(x, true_dist.clone().detach())\n",
    "    \n",
    "\n",
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss # return both total loss and avg loss\n",
    "    \n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "\n",
    "class TrainState:\n",
    "    step: int = 0\n",
    "    accum_step: int = 0\n",
    "    samples: int = 0\n",
    "    tokens: int = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data) # no need for padding mask during lazy decode\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state\n",
    "\n",
    "\n",
    "def example_simple_model():\n",
    "    V = 11\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(V, V, N=2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    batch_size = 80\n",
    "\n",
    "    # for testing\n",
    "    src = torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "    max_len = src.shape[1]\n",
    "    src_mask = torch.ones(1, 1, max_len)\n",
    "    train_state = TrainState()\n",
    "\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        _, train_state = run_epoch(\n",
    "            data_gen(V, batch_size, 20),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            train_state=train_state,\n",
    "            mode=\"train\"\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 5),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\"\n",
    "        )[0]\n",
    "        \n",
    "        print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_simple_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German to English translation task\n",
    "This is essentially the preprocessing step before the LLM.  \n",
    "First we get the tokenizer for the respective language. \n",
    "Then we construct the vocabulary size.  \n",
    "Finally we map the sentence to the vocabulary as an one hot encoding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "    lm = \"de_core_news_sm\"\n",
    "    try:\n",
    "        spacy_de = spacy.load(lm)\n",
    "    except OSError:\n",
    "        os.system(f\"python -m spacy download {lm}\")\n",
    "        spacy_de = spacy.load(lm)\n",
    "\n",
    "    lm = \"en_core_web_sm\"\n",
    "    try:\n",
    "        spacy_en = spacy.load(lm)\n",
    "    except OSError:\n",
    "        os.system(f\"python -m spacy download {lm}\")\n",
    "        spacy_en = spacy.load(lm)\n",
    "    \n",
    "    return spacy_de, spacy_en\n",
    "\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    for from_to_tuple in data_iter: # from_to refer to the original vs. translated sentences \n",
    "        yield tokenizer(from_to_tuple[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(spacy_de, spacy_en):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "    \n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "    \n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\")) # FIXME: there is some issue with the test dataset server so it cant be downloaded\n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val, tokenize_de, index=0), # index 0 is german\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val , tokenize_en, index=1), # index 1 is english\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_src.set_default_index(vocab_src[\"<unk>\"]) # out of vocabulary index\n",
    "    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n",
    "\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "def load_vocab(spacy_de, spacy_en):\n",
    "    if not os.path.exists(\"vocab.pt\"):\n",
    "        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n",
    "        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(len(vocab_src))\n",
    "    print(len(vocab_tgt))\n",
    "    return vocab_src, vocab_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de, spacy_en = load_tokenizers()\n",
    "vocab_src, vocab_tgt = load_vocab(spacy_de, spacy_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful examples\n",
    "for v in [\"<s>\", \"</s>\", \"<blank>\", \"ahbirf\", \"<unk>\", \"a\"]:\n",
    "    print(f\"{v}: {vocab_src[v]} {vocab_tgt[v]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collate batch adds sos and eos tokens to the input data pair, as well as max padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    batch_size=12000,\n",
    "    max_padding=128,\n",
    "):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(\n",
    "            batch,\n",
    "            tokenize_de,\n",
    "            tokenize_en,\n",
    "            vocab_src,\n",
    "            vocab_tgt,\n",
    "            max_padding=max_padding,\n",
    "            pad_id=vocab_src.get_stoi()[\"<blank>\"], # string to text \n",
    "        )\n",
    "\n",
    "    train_iter, valid_iter, test_iter = datasets.Multi30k(\n",
    "        language_pair=(\"de\", \"en\")\n",
    "    )\n",
    "\n",
    "    train_iter_map = to_map_style_dataset(\n",
    "        train_iter\n",
    "    ) # so the data can be indexed\n",
    "    valid_iter_map = to_map_style_dataset(valid_iter)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn, # necessary for a map styled iterator\n",
    "    )\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "\n",
    "def collate_batch(\n",
    "    batch,\n",
    "    src_pipeline, # the tokenizer\n",
    "    tgt_pipeline,\n",
    "    src_vocab, # the vocab\n",
    "    tgt_vocab,\n",
    "    max_padding=128,\n",
    "    pad_id=2,\n",
    "):\n",
    "    bs_id = torch.tensor([0])  # <s> token id\n",
    "    eos_id = torch.tensor([1])  # </s> token id\n",
    "    src_list, tgt_list = [], []\n",
    "    for (_src, _tgt) in batch:\n",
    "        processed_src = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    src_vocab(src_pipeline(_src)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=\"cpu\",\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        processed_tgt = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    tgt_vocab(tgt_pipeline(_tgt)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=\"cpu\",\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        src_list.append(\n",
    "            pad(\n",
    "                processed_src,\n",
    "                (\n",
    "                    0,\n",
    "                    max_padding - len(processed_src), # padded to the max length\n",
    "                ),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "        tgt_list.append(\n",
    "            pad(\n",
    "                processed_tgt,\n",
    "                (0, max_padding - len(processed_tgt)),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    src = torch.stack(src_list) # note the data structure here\n",
    "    tgt = torch.stack(tgt_list)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    config,\n",
    "):\n",
    "    pad_idx = vocab_tgt[\"<blank>\"]\n",
    "    d_model = 512\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    module = model\n",
    "\n",
    "    criterion = LabelSmoothing(\n",
    "        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        max_padding=config[\"max_padding\"],\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "    train_state = TrainState()\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch} Training ====\", flush=True)\n",
    "        _, train_state = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "        torch.save(module.state_dict(), file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval()\n",
    "        sloss = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "        print(sloss)\n",
    "\n",
    "    file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "    torch.save(module.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model():\n",
    "    config = {\n",
    "        \"batch_size\": 32,\n",
    "        \"num_epochs\": 5,\n",
    "        \"accum_iter\": 10,\n",
    "        \"base_lr\": 1.0,\n",
    "        \"max_padding\": 72,\n",
    "        \"warmup\": 3000,\n",
    "        \"file_prefix\": \"multi30k_model_\",\n",
    "    }\n",
    "    model_path = \"multi30k_model_final.pt\"\n",
    "    if not os.path.exists(model_path):\n",
    "        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n",
    "\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
